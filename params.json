{"name":"Surf-example-page","tagline":"","body":"#Examples\r\n* **Example 1** - Simple Setup [here](http://bbnow.github.io/testing/index1.html#example_1)\r\n* **Example 2** - Page Count [here](http://bbnow.github.io/testing/index1.html#example_2)\r\n\r\n***\r\n\r\n<a name=\"example_1\">\r\n##Example 1 – Simple Setup\r\nThis example explores connectivity between on-premise data sources and cloud-based Kinesis applications.\r\n\r\n![Example1 Picture](images/example_1_1.png)\r\n\r\nExample's scenario is very simple – a local data source generates a stream of time-stamps, the cloud application displays the time-stamps on a console.\r\n\r\n###Surf Data Source\r\nThe code for the data source can be found in file `com.informatica.surf.sources.dummy.DummySource.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sources/dummy/src/main/java/com/informatica/surf/sources/dummy/DummySource.java). \r\n\r\nFor this example the data source generates time-stamps and streams them to a Kinesis application. Here is the code snippet performing this operation:\r\n    \r\n    public void read(VDSEventList readEvents) throws Exception {\r\n         Thread.sleep(1000);\r\n         Date d = new Date();\r\n         byte []b = d.toString().getBytes();\r\n         readEvents.addEvent(b, b.length, _headers);\r\n    } \r\n\r\n> Generally, Surf users do not need to develop code in order for Surf to access a data source. They only configure Surf with a few properties for File-tailer, HTTP, or MQTT. However, if needed users may also easily develop logic for a new data source based on existing examples.\r\n\r\n###Surf Application\r\nSource code for a simple Surf application used in this example is located in `com.informatica.surf.sample.DumpStream.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/DumpStream.java).\r\n\r\nIn this case the application logic is very simple – take a record from Kinesis stream and print it. Here is the code snippet:\r\n\r\n    public void onEvent(KinesisEvent kinesisEvent, long l, boolean b) throws Exception {    \r\n        System.out.println(String.format(\"Received : %s\", kinesisEvent.getData()));\r\n    }\r\n\r\nApplication developer only provides a handler - _onEvent_ – and includes the application logic within this handler.\r\n\r\n###Configuration Steps\r\n\r\nPlease, follow the following steps to configure the system:<br>\r\n1. Enable Kinesis Stream<br>\r\n2. Configure and start Surf application<br>\r\n3. Configure and start Surf data source<br>\r\n4. Observe system in action<br>\r\n\r\n**Enabling Kinesis Stream**<br>\r\nFirst we need to create a Kinesis stream. Do this by logging into [AWS Kinesis console](https://console.aws.amazon.com/kinesis/?#). Instructions can be found [here](http://docs.aws.amazon.com/kinesis/latest/dev/step-one-create-stream.html).\r\n\r\n![Kinesis Stream Create Picture](images/example_1_2.png)\r\n\r\nRemember _Kinesis stream name_ as it is used for configuration of Surf processes.\r\n\r\n**Configuring Surf Application**<br>\r\nBring up an EC2 instance and copy Surf distribution package to it. Details on starting an EC2 instance and interacting with it can be found [here](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html).\r\n\r\nWe will refer to the top-level directory of Surf distribution as _**surf-home**_.\r\nThe distribution package can be found in _assembly/target/surf-1.0-dist_ directory of the Surf repository tree once it is built by Maven. Instructions for Surf build process are [here](https://github.com/InformaticaCorp/Surf#build).\r\n\r\nThe next step is to create a configuration file that contain AWS credentials and _Kinesis stream name_. The file should reside in _**surf-home**/conf_ directory. \r\nLet’s name the file _app-node1.config_ and place the following entries into it:\r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n\r\nNext, make sure that Java 7 JRE is available and is configured properly. Note that not only the right JRE should be available, but also JAVA_HOME should be pointing to JRE’s home directory.  \r\n\r\nNow, the application is ready to be started with the following command line:\r\n\r\n    ./surf.sh dump-stream app-node1\r\n\r\nWhile no output is yet displayed by the application, the application log file can be found in _**surf-home**/log_ directory. The file will contain information about application start up and current listening state.\r\n\r\n**Configuring Surf Data Source**<br>\r\nCopy the surf distribution package to your local machine and ensure Java 7 availability.\r\nSimilarly to the application, let’s create a configuration file _source-node1.conf_ in _**surf-home**/conf_ directory containing the following entries: \r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n    vds-source-class: com.informatica.surf.sources.dummy.DummySource\r\n\r\nNow we are ready to start the data source Surf process with the following command in _**surf-home**/bin_ directory:\r\n\r\n    ./surf.sh start-node source-node1\r\n\r\nAs the process starts, we can examine the log file found in _**surf-home**/log_ directory.\r\n\r\n**Observing System in Action**<br>\r\nAs the data source comes on-line, the Surf application comes to life. You can see the stream of time-stamps appearing on the application console. \r\n\r\n![Example1 Output Picture](images/example_1_3.png)\r\n\r\nYou can now extend the sample in several ways:\r\n* Start another data source process and observe the number of messages flowing through the system increase\r\n* Extend the data source (DummySource.java) to insert another field in the record that would identify it (process id or a UUID) and observe the identifier displayed on the application output console   \r\n\r\n<a name=\"example_2\">\r\n##Example 2 – Page Counts\r\nIn this example we explore a more interesting data source and application logic. The example illustrates how Surf can monitor a weblog file for updates and stream newly added records to a Kinesis application.\r\n\r\n![Example2 Picture](images/example_2_1.png)\r\n\r\nThe log file in this example contains information about web page visits. You can download a sample web log from this website - [http://ita.ee.lbl.gov/html/traces.html](http://ita.ee.lbl.gov/html/traces.html). We took _epa-http.txt_ file for testing.\r\n\r\nRecords in the file look as follows:\r\n\r\n    query2.lycos.cs.cmu.edu [29:23:53:36] \"GET /Consumer.html HTTP/1.0\" 200 1325\r\n    tanuki.twics.com [29:23:53:53] \"GET /News.html HTTP/1.0\" 200 1014\r\n    wpbfl2-45.gate.net [29:23:54:15] \"GET / HTTP/1.0\" 200 4889\r\n    wpbfl2-45.gate.net [29:23:54:16] \"GET /icons/circle_logo_small.gif HTTP/1.0\" 200 2624\r\n    wpbfl2-45.gate.net [29:23:54:18] \"GET /logos/small_gopher.gif HTTP/1.0\" 200 935\r\n\r\nSurf application logic keeps count of how many times each particular page has been visited. \r\n\r\n###Configuring Surf Application\r\nSource code for the application can be found in files `com.informatica.surf.sample.PageCount.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/PageCount.java) and `com.informatica.surf.sample.PageCountHandler.java` [here](https://github.com/InformaticaCorp/Surf/blob/master/sample/src/main/java/com/informatica/surf/sample/PageCountHandler.java). Just like in the simple example before, all application logic resides in _onEvent_ handler. The logic deals with taking records off the stream, parsing them to extract the page URL, and then keeping count of how many times each URL has been referenced:   \r\n\r\n    public void onEvent(KinesisEvent t, long l, boolean bln) throws Exception {\r\n        String data = t.getData();\r\n        _logger.debug(\"Got an event: {}\", data);\r\n        // data may consist of multiple lines\r\n        StringTokenizer tok = new StringTokenizer(data, \"\\n\");\r\n        while(tok.hasMoreTokens()){\r\n            String line = tok.nextToken();\r\n            _logger.debug(\"Line = {}\", line);\r\n            Matcher m = _pattern.matcher(line);\r\n            if(m.matches()){\r\n                String page = m.group(1);\r\n                _logger.debug(\"Found page {}\", page);\r\n                if(page != null){\r\n                    _counts.add(page);\r\n                }\r\n            }\r\n            else{\r\n                _logger.debug(\"Unmatched log line\");\r\n            }\r\n        }\r\n    }\r\n\r\nApplication configuration is identical to the previous example. We, therefore, don't need to change the configuration file.\r\n\r\nIn order to start the application we may use the same Surf command with changed parameters:\r\n\r\n    ./surf.sh page-count app-node1\r\n\r\n###Configuring Surf Data Source\r\nThe data source for this example is a growing weblog file. Let’s create a new directory _**surf-home**/data_ and copy the test data - _epa-http.txt_ file.  \r\n\r\nNext, let’s create a different data source configuration file for this scenario. File  _source-node2.conf_ should contain the following entries:\r\n\r\n    aws-access-key-id: <your aws access key>\r\n    aws-secret-key: <your aws secret key>\r\n    aws-kinesis-stream-name: <kinesis stream name>\r\n    vds-source-class: com.informatica.binge.sources.file.BingeFileReader\r\n    flight_size: 10\r\n    directory: ../data\r\n    filename:  test_data.txt\r\n\r\nBecause Surf monitors data file for inserted records, we need to take a few steps to enable the record stream while being in _**surf-home**/data_ directory:\r\n\r\n    $touch test_data.txt\r\n    $../bin/surf.sh start-node source-node2.conf\r\n    $cat epa-http.txt >> test_data.txt\r\n\r\nNow the Surf data source will transfer a few records at a time to the page-count application.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}